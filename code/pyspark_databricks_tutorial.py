# Databricks notebook source
# MAGIC %md
# MAGIC # PhÃ¢n tÃ­ch dá»¯ liá»‡u sá»­ dá»¥ng Pyspark trÃªn Databricks

# COMMAND ----------

# MAGIC %md
# MAGIC %md
# MAGIC ## Tá»•ng quan
# MAGIC PySpark lÃ  API Python cá»§a Apache Spark, má»™t framework Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u quy mÃ´ lá»›n má»™t cÃ¡ch hiá»‡u quáº£ trong mÃ´i trÆ°á»ng phÃ¢n tÃ¡n. Apache Spark lÃ  má»™t framework mÃ£ nguá»“n má»Ÿ cho cluster computing, cÃ³ thá»ƒ thá»±c hiá»‡n nhanh chÃ³ng viá»‡c xá»­ lÃ½ dá»¯ liá»‡u, xá»­ lÃ½ streaming, machine learning, xá»­ lÃ½ Ä‘á»“ thá»‹, v.v.
# MAGIC
# MAGIC ## Táº¡i sao sá»­ dá»¥ng PySpark?
# MAGIC - **Xá»­ lÃ½ dá»¯ liá»‡u quy mÃ´ lá»›n**: PySpark xá»­ lÃ½ phÃ¢n tÃ¡n khá»‘i lÆ°á»£ng dá»¯ liá»‡u lá»›n, tá»‘c Ä‘á»™ xá»­ lÃ½ nhanh vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng trÃªn cluster.
# MAGIC - **API linh hoáº¡t**: PySpark cung cáº¥p API Python thÃ¢n thiá»‡n, cÃ³ thá»ƒ Ä‘Ã¡p á»©ng tá»« cÃ¡c thao tÃ¡c Ä‘Æ¡n giáº£n nhÆ° Pandas Ä‘áº¿n xá»­ lÃ½ phÃ¢n tÃ¡n phá»©c táº¡p.
# MAGIC - **á»¨ng dá»¥ng rá»™ng rÃ£i**: ÄÃ¡p á»©ng nhiá»u má»¥c Ä‘Ã­ch khÃ¡c nhau nhÆ° ETL (Extract, Transform, Load), phÃ¢n tÃ­ch dá»¯ liá»‡u, machine learning, real-time streaming, v.v.
# MAGIC
# MAGIC ## CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a PySpark
# MAGIC 1. **RDD (Resilient Distributed Dataset)**: Cáº¥u trÃºc dá»¯ liá»‡u cÆ¡ báº£n, lÃ  táº­p há»£p dá»¯ liá»‡u báº¥t biáº¿n Ä‘Æ°á»£c phÃ¢n tÃ¡n. Thao tÃ¡c báº±ng transformation vÃ  action.
# MAGIC 2. **DataFrame**: API cáº¥p cao Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc, cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c thao tÃ¡c giá»‘ng SQL. CÃ³ cÃ¡ch sá»­ dá»¥ng tÆ°Æ¡ng tá»± DataFrame cá»§a Pandas vÃ  cÃ³ thá»ƒ thá»±c thi SQL query.
# MAGIC 3. **Spark SQL**: Module cÃ³ thá»ƒ thao tÃ¡c DataFrame báº±ng SQL query. CÃ³ thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u giá»‘ng nhÆ° database.
# MAGIC 4. **MLlib**: ThÆ° viá»‡n cung cáº¥p cÃ¡c thuáº­t toÃ¡n machine learning, cÃ³ thá»ƒ thá»±c hiá»‡n distributed learning trong cluster.
# MAGIC 5. **Spark Streaming**: Module Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u real-time. Xá»­ lÃ½ dá»¯ liá»‡u real-time báº±ng micro-batch.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup vÃ  Import Libraries

# COMMAND ----------

from datetime import datetime, timedelta

import pyspark.sql.functions as F
import pyspark.sql.types as T
import pyspark.sql.window as W
from pyspark.sql import SparkSession

# Táº¡o hoáº·c láº¥y Spark session (thÆ°á»ng Ä‘Ã£ cÃ³ sáºµn trÃªn Databricks)
# spark = SparkSession.builder.appName("PySpark_Tutorial").getOrCreate()

print("PySpark setup hoÃ n táº¥t!")
print(f"Spark version: {spark.version}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Táº¡o vÃ  hiá»ƒn thá»‹ DataFrame
# MAGIC
# MAGIC DataFrame lÃ  cáº¥u trÃºc dá»¯ liá»‡u chÃ­nh trong PySpark, tÆ°Æ¡ng tá»± nhÆ° báº£ng trong SQL.

# COMMAND ----------

# === Táº O DATAFRAME Tá»ª Dá»® LIá»†U MáºªU ===

# Dá»¯ liá»‡u bÃ¡n hÃ ng máº«u
sales_data = [
    ("TXN001", "C001", "P001", 2, 1000.0, "2024-01-15", "S001"),
    ("TXN002", "C002", "P002", 1, 5000.0, "2024-01-15", "S002"),
    ("TXN003", "C003", "P003", 3, 3000.0, "2024-01-16", "S001"),
    ("TXN004", "C001", "P004", 1, 2000.0, "2024-01-17", "S003"),
    ("TXN005", "C004", "P001", 5, 1000.0, "2024-01-17", "S002"),
    ("TXN006", "C002", "P005", 2, 1200.0, "2024-01-18", "S001"),
    ("TXN007", "C005", "P002", 1, 5000.0, "2024-01-18", "S003"),
    ("TXN008", "C003", "P003", 4, 3000.0, "2024-01-19", "S002"),
]

# Äá»‹nh nghÄ©a schema (cáº¥u trÃºc dá»¯ liá»‡u)
sales_schema = T.StructType(
    [
        T.StructField("transaction_id", T.StringType(), True),
        T.StructField("customer_id", T.StringType(), True),
        T.StructField("product_id", T.StringType(), True),
        T.StructField("quantity", T.IntegerType(), True),
        T.StructField("unit_price", T.DoubleType(), True),
        T.StructField("sale_date", T.StringType(), True),
        T.StructField("store_id", T.StringType(), True),
    ]
)

# Táº¡o DataFrame
sdf_sales = spark.createDataFrame(sales_data, sales_schema)

print("THÃ”NG TIN DATAFRAME:")
print(f"Sá»‘ dÃ²ng: {sdf_sales.count()}")
print(f"Sá»‘ cá»™t: {len(sdf_sales.columns)}")
print(f"TÃªn cÃ¡c cá»™t: {sdf_sales.columns}")

# COMMAND ----------

# === HIá»‚N THá»Š DATAFRAME ===

# CÃ¡ch 1: show() - hiá»ƒn thá»‹ dáº¡ng text
print("Hiá»ƒn thá»‹ vá»›i .show():")
sdf_sales.show(5)  # Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u

# CÃ¡ch 2: display() - hiá»ƒn thá»‹ Ä‘áº¹p máº¯t (chá»‰ cÃ³ trÃªn Databricks)
print("Hiá»ƒn thá»‹ vá»›i .display():")
sdf_sales.display()

# Xem schema (cáº¥u trÃºc dá»¯ liá»‡u)
print("Schema cá»§a DataFrame:")
sdf_sales.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Thao tÃ¡c vá»›i cá»™t
# MAGIC
# MAGIC CÃ¡c thao tÃ¡c phá»• biáº¿n: select, add, drop, rename columns

# COMMAND ----------

# === CHá»ŒN Cá»˜T ===

# Chá»n má»™t vÃ i cá»™t cá»¥ thá»ƒ
print("Chá»n cá»™t customer_id vÃ  product_id:")
sdf_sales.select("customer_id", "product_id").display(3)

# Chá»n táº¥t cáº£ cá»™t
sdf_sales.select("*").display(3)

# Láº¥y danh sÃ¡ch tÃªn cá»™t
column_names = sdf_sales.columns
print(f"Danh sÃ¡ch cá»™t: {column_names}")

# COMMAND ----------

# === THÃŠM Cá»˜T Má»šI ===

# ThÃªm cá»™t vá»›i giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh
sdf_sales_with_country = sdf_sales.withColumn("country", F.lit("Vietnam"))

# ThÃªm cá»™t tÃ­nh toÃ¡n tá»« cÃ¡c cá»™t khÃ¡c
sdf_sales_enhanced = sdf_sales_with_country.withColumn(
    "total_amount", F.col("quantity") * F.col("unit_price")
)

# ThÃªm cá»™t vá»›i logic Ä‘iá»u kiá»‡n
sdf_sales_enhanced = sdf_sales_enhanced.withColumn(
    "order_size",
    F.when(F.col("quantity") >= 5, "Large")
    .when(F.col("quantity") >= 3, "Medium")
    .otherwise("Small"),
)

print("DataFrame sau khi thÃªm cá»™t:")
sdf_sales_enhanced.display()

# COMMAND ----------

# === Äá»”I TÃŠN VÃ€ XÃ“A Cá»˜T ===

# Äá»•i tÃªn cá»™t
sdf_sales_renamed = sdf_sales_enhanced.withColumnRenamed("sale_date", "purchase_date")

# XÃ³a cá»™t khÃ´ng cáº§n thiáº¿t
sdf_sales_cleaned = sdf_sales_renamed.drop("country")  # XÃ³a cá»™t country vá»«a táº¡o

print("DataFrame sau khi Ä‘á»•i tÃªn vÃ  xÃ³a cá»™t:")
sdf_sales_cleaned.display()

# COMMAND ----------

# === Sá»¬ Dá»¤NG FUNCTIONS Vá»šI Cá»˜T ===

# CÃ¡c hÃ m xá»­ lÃ½ chuá»—i
sdf_string_ops = sdf_sales.withColumn("product_upper", F.upper("product_id"))
sdf_string_ops = sdf_string_ops.withColumn(
    "customer_product", F.concat("customer_id", F.lit("_"), "product_id")
)

# CÃ¡c hÃ m xá»­ lÃ½ ngÃ y thÃ¡ng (chuyá»ƒn string thÃ nh date trÆ°á»›c)
sdf_date_ops = sdf_sales.withColumn("sale_date", F.to_date("sale_date", "yyyy-MM-dd"))
sdf_date_ops = sdf_date_ops.withColumn("year", F.year("sale_date"))
sdf_date_ops = sdf_date_ops.withColumn("month", F.month("sale_date"))
sdf_date_ops = sdf_date_ops.withColumn("day_of_week", F.dayofweek("sale_date"))

print("Thao tÃ¡c vá»›i chuá»—i vÃ  ngÃ y thÃ¡ng:")
sdf_date_ops.select("sale_date", "year", "month", "day_of_week").display()

# COMMAND ----------

# === USER DEFINED FUNCTION (UDF) ===


# Äá»‹nh nghÄ©a hÃ m Python tÃ¹y chá»‰nh
def calculate_discount(quantity, unit_price):
    """TÃ­nh discount dá»±a trÃªn quantity"""
    total = quantity * unit_price
    if total >= 10000:
        return total * 0.1  # 10% discount
    elif total >= 5000:
        return total * 0.05  # 5% discount
    else:
        return 0


# Chuyá»ƒn thÃ nh UDF Ä‘á»ƒ dÃ¹ng trong PySpark
discount_udf = F.udf(calculate_discount, T.DoubleType())

# Ãp dá»¥ng UDF
sdf_sales_with_discount = sdf_sales.withColumn(
    "discount", discount_udf(F.col("quantity"), F.col("unit_price"))
)

print("DataFrame vá»›i discount tÃ­nh báº±ng UDF:")
sdf_sales_with_discount.display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Thao tÃ¡c vá»›i dÃ²ng
# MAGIC
# MAGIC Filter, sort, vÃ  xá»­ lÃ½ missing values

# COMMAND ----------

# === Lá»ŒC Dá»® LIá»†U (FILTER) ===

# Lá»c vá»›i Ä‘iá»u kiá»‡n Ä‘Æ¡n giáº£n
sdf_high_quantity = sdf_sales.filter(F.col("quantity") >= 3)
print("ÄÆ¡n hÃ ng cÃ³ quantity >= 3:")
sdf_high_quantity.display()

# Lá»c vá»›i nhiá»u Ä‘iá»u kiá»‡n (AND)
sdf_expensive_large_orders = sdf_sales.filter(
    (F.col("quantity") >= 2) & (F.col("unit_price") >= 2000)
)
print("ÄÆ¡n hÃ ng lá»›n vÃ  Ä‘áº¯t tiá»n:")
sdf_expensive_large_orders.display()

# Lá»c vá»›i Ä‘iá»u kiá»‡n OR
sdf_specific_customers = sdf_sales.filter(
    (F.col("customer_id") == "C001") | (F.col("customer_id") == "C002")
)
print("ÄÆ¡n hÃ ng cá»§a khÃ¡ch hÃ ng C001 hoáº·c C002:")
sdf_specific_customers.display()

# Lá»c vá»›i danh sÃ¡ch giÃ¡ trá»‹
sdf_selected_products = sdf_sales.filter(
    F.col("product_id").isin(["P001", "P002", "P003"])
)
print("ÄÆ¡n hÃ ng cá»§a sáº£n pháº©m P001, P002, P003:")
sdf_selected_products.display()

# COMMAND ----------

# === Sáº®P Xáº¾P Dá»® LIá»†U ===

# Sáº¯p xáº¿p tÄƒng dáº§n
sdf_sorted_asc = sdf_sales.orderBy("unit_price")
print("Sáº¯p xáº¿p theo unit_price tÄƒng dáº§n:")
sdf_sorted_asc.display()

# Sáº¯p xáº¿p giáº£m dáº§n
sdf_sorted_desc = sdf_sales.orderBy(F.desc("unit_price"))
print("Sáº¯p xáº¿p theo unit_price giáº£m dáº§n:")
sdf_sorted_desc.display()

# Sáº¯p xáº¿p nhiá»u cá»™t
sdf_multi_sort = sdf_sales.orderBy(["customer_id", F.desc("unit_price")])
print("Sáº¯p xáº¿p theo customer_id tÄƒng, unit_price giáº£m:")
sdf_multi_sort.display()

# COMMAND ----------

# === Xá»¬ LÃ MISSING VALUES ===

# Táº¡o DataFrame cÃ³ giÃ¡ trá»‹ null Ä‘á»ƒ demo
sdf_with_nulls = sdf_sales.withColumn(
    "discount_rate",
    F.when(F.col("product_id") == "P001", 0.1)
    .when(F.col("product_id") == "P002", None)  # Null value
    .when(F.col("product_id") == "P003", 0.15)
    .otherwise(None),
)

print("DataFrame cÃ³ giÃ¡ trá»‹ null:")
sdf_with_nulls.select("product_id", "discount_rate").display()

# Äáº¿m giÃ¡ trá»‹ null
null_count = sdf_with_nulls.filter(F.col("discount_rate").isNull()).count()
print(f"Sá»‘ dÃ²ng cÃ³ discount_rate null: {null_count}")

# Bá»• sung giÃ¡ trá»‹ null
sdf_filled = sdf_with_nulls.fillna(0.05, subset=["discount_rate"])
print("Sau khi bá»• sung null báº±ng 0.05:")
sdf_filled.select("product_id", "discount_rate").display()

# XÃ³a dÃ²ng cÃ³ giÃ¡ trá»‹ null
sdf_dropped = sdf_with_nulls.dropna(subset=["discount_rate"])
print("Sau khi xÃ³a dÃ²ng cÃ³ null:")
sdf_dropped.select("product_id", "discount_rate").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. NhÃ³m hÃ³a vÃ  tá»•ng há»£p (GroupBy & Aggregation)
# MAGIC
# MAGIC TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ thá»‘ng kÃª theo nhÃ³m

# COMMAND ----------

# === NHÃ“M HÃ“A CÆ  Báº¢N ===

# TÃ­nh toÃ¡n cÆ¡ báº£n theo tá»«ng nhÃ³m
sdf_customer_stats = sdf_sales.groupBy("customer_id").agg(
    F.sum("quantity").alias("total_quantity"),
    F.avg("unit_price").alias("avg_price"),
    F.count("transaction_id").alias("transaction_count"),
    F.min("unit_price").alias("min_price"),
    F.max("unit_price").alias("max_price"),
)

print("Thá»‘ng kÃª theo khÃ¡ch hÃ ng:")
sdf_customer_stats.display()

# COMMAND ----------

# === NHÃ“M HÃ“A NÃ‚NG CAO ===

# TÃ­nh tá»•ng doanh thu vÃ  thá»‘ng kÃª chi tiáº¿t theo cá»­a hÃ ng
sdf_store_performance = (
    sdf_sales.withColumn("total_amount", F.col("quantity") * F.col("unit_price"))
    .groupBy("store_id")
    .agg(
        F.sum("total_amount").alias("total_revenue"),
        F.avg("total_amount").alias("avg_order_value"),
        F.count("transaction_id").alias("total_orders"),
        F.countDistinct("customer_id").alias("unique_customers"),
        F.countDistinct("product_id").alias("unique_products"),
        F.stddev("total_amount").alias("revenue_std_dev"),
        F.first("store_id").alias("store_code"),  # Just for demo
    )
    .orderBy(F.desc("total_revenue"))
)

print("Hiá»‡u suáº¥t theo cá»­a hÃ ng:")
sdf_store_performance.display()

# COMMAND ----------

# === Tá»”NG Há»¢P CÃ“ ÄIá»€U KIá»†N ===

# TÃ­nh tá»•ng cÃ³ Ä‘iá»u kiá»‡n sá»­ dá»¥ng CASE WHEN
sdf_conditional_aggregation = (
    sdf_sales.withColumn("total_amount", F.col("quantity") * F.col("unit_price"))
    .groupBy("customer_id")
    .agg(
        # Tá»•ng sá»‘ Ä‘Æ¡n hÃ ng lá»›n (>= 5000)
        F.sum(F.when(F.col("total_amount") >= 5000, 1).otherwise(0)).alias(
            "large_orders"
        ),
        # Tá»•ng doanh thu tá»« Ä‘Æ¡n hÃ ng lá»›n
        F.sum(
            F.when(F.col("total_amount") >= 5000, F.col("total_amount")).otherwise(0)
        ).alias("large_order_revenue"),
        # Tá»•ng doanh thu
        F.sum("total_amount").alias("total_revenue"),
    )
    .withColumn(
        "large_order_percentage",
        F.round((F.col("large_order_revenue") / F.col("total_revenue")) * 100, 2),
    )
)

print("PhÃ¢n tÃ­ch Ä‘Æ¡n hÃ ng lá»›n theo khÃ¡ch hÃ ng:")
sdf_conditional_aggregation.display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Window Functions
# MAGIC
# MAGIC Thá»±c hiá»‡n tÃ­nh toÃ¡n trÃªn "cá»­a sá»•" dá»¯ liá»‡u mÃ  khÃ´ng cáº§n groupby

# COMMAND ----------

# === RANKING FUNCTIONS ===

# Táº¡o cá»™t total_amount Ä‘á»ƒ ranking
sdf_sales_with_total = sdf_sales.withColumn(
    "total_amount", F.col("quantity") * F.col("unit_price")
)

# Äá»‹nh nghÄ©a Window specification
customer_window = W.Window.partitionBy("customer_id").orderBy(F.desc("total_amount"))
overall_window = W.Window.orderBy(F.desc("total_amount"))

# Ãp dá»¥ng cÃ¡c hÃ m ranking
sdf_ranked = (
    sdf_sales_with_total.withColumn("rank_in_customer", F.rank().over(customer_window))
    .withColumn("dense_rank_in_customer", F.dense_rank().over(customer_window))
    .withColumn("row_number_in_customer", F.row_number().over(customer_window))
    .withColumn("overall_rank", F.rank().over(overall_window))
)

print("Ranking cÃ¡c giao dá»‹ch:")
sdf_ranked.select(
    "customer_id",
    "total_amount",
    "rank_in_customer",
    "dense_rank_in_customer",
    "overall_rank",
).display()

# COMMAND ----------

# === WINDOW AGGREGATE FUNCTIONS ===

# TÃ­nh tá»•ng running totals vÃ  moving averages
date_window = W.Window.orderBy("sale_date")
customer_window_unbounded = W.Window.partitionBy("customer_id")

sdf_window_agg = (
    sdf_sales_with_total.withColumn("sale_date", F.to_date("sale_date", "yyyy-MM-dd"))
    .withColumn("running_total", F.sum("total_amount").over(date_window))
    .withColumn("customer_total", F.sum("total_amount").over(customer_window_unbounded))
    .withColumn("customer_avg", F.avg("total_amount").over(customer_window_unbounded))
)

print("Window aggregations:")
sdf_window_agg.select(
    "customer_id", "sale_date", "total_amount", "running_total", "customer_total"
).orderBy("sale_date").display()

# COMMAND ----------

# === MOVING AVERAGES ===

# Moving average 3 giao dá»‹ch gáº§n nháº¥t
moving_window = W.Window.orderBy("sale_date").rowsBetween(-2, 0)

sdf_moving_avg = (
    sdf_sales_with_total.withColumn("sale_date", F.to_date("sale_date", "yyyy-MM-dd"))
    .withColumn("moving_avg_3", F.avg("total_amount").over(moving_window))
    .withColumn("moving_sum_3", F.sum("total_amount").over(moving_window))
)

print("Moving averages (3 giao dá»‹ch gáº§n nháº¥t):")
sdf_moving_avg.select(
    "sale_date", "total_amount", "moving_avg_3", "moving_sum_3"
).orderBy("sale_date").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Káº¿t há»£p DataFrame (Joins)
# MAGIC
# MAGIC Káº¿t há»£p dá»¯ liá»‡u tá»« nhiá»u báº£ng khÃ¡c nhau

# COMMAND ----------

# === CHUáº¨N Bá»Š Dá»® LIá»†U CHO JOIN ===

# Táº¡o báº£ng thÃ´ng tin khÃ¡ch hÃ ng
customers_data = [
    ("C001", "Nguyen Van A", "VIP", "Ha Noi"),
    ("C002", "Tran Thi B", "Gold", "Ho Chi Minh"),
    ("C003", "Le Van C", "Silver", "Da Nang"),
    ("C004", "Pham Thi D", "VIP", "Ha Noi"),
    ("C005", "Hoang Van E", "Gold", "Hai Phong"),
    ("C006", "Mai Van F", "Silver", "Can Tho"),  # KhÃ¡ch hÃ ng khÃ´ng cÃ³ giao dá»‹ch
]

customers_schema = T.StructType(
    [
        T.StructField("customer_id", T.StringType(), True),
        T.StructField("customer_name", T.StringType(), True),
        T.StructField("tier", T.StringType(), True),
        T.StructField("city", T.StringType(), True),
    ]
)

sdf_customers = spark.createDataFrame(customers_data, customers_schema)

# Táº¡o báº£ng thÃ´ng tin sáº£n pháº©m
products_data = [
    ("P001", "iPhone 15", "Electronics", 15000.0),
    ("P002", "Samsung S24", "Electronics", 25000.0),
    ("P003", "Nike Shoes", "Fashion", 8000.0),
    ("P004", "MacBook Pro", "Electronics", 45000.0),
    ("P005", "Adidas Shirt", "Fashion", 1200.0),
]

products_schema = T.StructType(
    [
        T.StructField("product_id", T.StringType(), True),
        T.StructField("product_name", T.StringType(), True),
        T.StructField("category", T.StringType(), True),
        T.StructField("list_price", T.DoubleType(), True),
    ]
)

sdf_products = spark.createDataFrame(products_data, products_schema)

print("Báº£ng khÃ¡ch hÃ ng:")
sdf_customers.display()
print("Báº£ng sáº£n pháº©m:")
sdf_products.display()

# COMMAND ----------

# === CÃC LOáº I JOIN ===

# 1. INNER JOIN - Chá»‰ láº¥y dá»¯ liá»‡u cÃ³ match á»Ÿ cáº£ 2 bÃªn
sdf_inner = sdf_sales.join(sdf_customers, on="customer_id", how="inner")
print("INNER JOIN (Sales + Customers):")
sdf_inner.select(
    "transaction_id", "customer_id", "customer_name", "tier", "product_id"
).display()

# 2. LEFT JOIN - Giá»¯ táº¥t cáº£ dá»¯ liá»‡u bÃªn trÃ¡i
sdf_left = sdf_sales.join(sdf_customers, on="customer_id", how="left")
print("LEFT JOIN (giá»¯ táº¥t cáº£ sales):")
print(f"Sá»‘ dÃ²ng sales gá»‘c: {sdf_sales.count()}, sau left join: {sdf_left.count()}")
sdf_left.display()

# 3. RIGHT JOIN - Giá»¯ táº¥t cáº£ dá»¯ liá»‡u bÃªn pháº£i
sdf_right = sdf_sales.join(sdf_customers, on="customer_id", how="right")
print("RIGHT JOIN (giá»¯ táº¥t cáº£ customers):")
print(
    f"Sá»‘ dÃ²ng customers: {sdf_customers.count()}, sau right join: {sdf_right.count()}"
)
# sdf_right.filter(F.col("transaction_id").isNull()).display()  # Customers khÃ´ng cÃ³ giao dá»‹ch
sdf_right.display()

# 4. FULL OUTER JOIN - Giá»¯ táº¥t cáº£ dá»¯ liá»‡u tá»« cáº£ 2 bÃªn
sdf_full = sdf_sales.join(sdf_customers, on="customer_id", how="outer")
print("FULL OUTER JOIN:")
print(f"Sá»‘ dÃ²ng sau full join: {sdf_full.count()}")
sdf_full.display()

# COMMAND ----------

# === JOIN NHIá»€U Báº¢NG ===

# Káº¿t há»£p cáº£ 3 báº£ng: Sales + Customers + Products
sdf_complete = sdf_sales.join(sdf_customers, "customer_id", "left").join(
    sdf_products, "product_id", "left"
)

print("Dá»¯ liá»‡u hoÃ n chá»‰nh (3 báº£ng):")
sdf_complete.select(
    "transaction_id",
    "customer_name",
    "product_name",
    "category",
    "quantity",
    "unit_price",
    "tier",
).display()

# TÃ­nh toÃ¡n trÃªn dá»¯ liá»‡u Ä‘Ã£ join
sdf_analysis = sdf_complete.withColumn(
    "total_amount", F.col("quantity") * F.col("unit_price")
).withColumn(
    "discount_amount",
    F.when(F.col("tier") == "VIP", F.col("total_amount") * 0.1)
    .when(F.col("tier") == "Gold", F.col("total_amount") * 0.05)
    .otherwise(0),
)

print("PhÃ¢n tÃ­ch vá»›i discount theo tier:")
sdf_analysis.select(
    "customer_name", "tier", "total_amount", "discount_amount"
).display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. LÆ°u trá»¯

# COMMAND ----------

# LÆ°u dÆ°á»›i dáº¡ng CSV
# output_path_csv = "/path/to/save/sdf_output.csv"
# sdf_sales.write.csv(output_path_csv, header=True, mode="overwrite")
# sdf_sales.write.csv(output_path_csv, header=True, mode="append")

# COMMAND ----------

# MAGIC %md
# MAGIC Khi dá»¯ liá»‡u Ä‘Ã£ tá»“n táº¡i, cÃ³ thá»ƒ ghi Ä‘Ã¨ dá»¯ liá»‡u báº±ng cÃ¡ch chá»‰ Ä‘á»‹nh `mode` thÃ nh `overwrite`

# COMMAND ----------

# # LÆ°u dÆ°á»›i dáº¡ng Parquet
# output_path_parquet = "/path/to/save/sdf_output.parquet"
# sdf_sales.write.parquet(output_path_parquet, mode="overwrite")

# COMMAND ----------

# # LÆ°u dÆ°á»›i dáº¡ng JSON
# output_path_json = "/path/to/save/sdf_output.json"
# sdf_sales.write.json(output_path_json, mode="overwrite")

# COMMAND ----------

# LÆ°u dÆ°á»›i dáº¡ng table
sdf_left.write.saveAsTable("sales_table", mode="overwrite")

# COMMAND ----------

# MAGIC %md
# MAGIC * LÆ°u DataFrame thÃ nh nhiá»u file. ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ quáº£n lÃ½ kÃ­ch thÆ°á»›c file dá»… dÃ ng hÆ¡n khi xá»­ lÃ½ DataFrame quy mÃ´ lá»›n.
# MAGIC   * Sá»­ dá»¥ng `repartition(10)` Ä‘á»ƒ chia DataFrame thÃ nh 10 partition.
# MAGIC   * File CSV riÃªng biá»‡t Ä‘Æ°á»£c lÆ°u cho má»—i partition Ä‘Æ°á»£c chia.

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM workspace.default.sales_table

# COMMAND ----------

# # LÆ°u dÆ°á»›i dáº¡ng CSV Ä‘Æ°á»£c chia thÃ nh nhiá»u file
# output_path_split = "/path/to/save/sdf_output_split"
# sdf_sales.repartition(10).write.csv(output_path_split, header=True, mode="overwrite")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Chuyá»ƒn Ä‘á»•i Ä‘á»‘i tÆ°á»£ng

# COMMAND ----------

# MAGIC %md
# MAGIC * `pyspark.sql.dataframe.DataFrame` -> `pandas.DataFrame`

# COMMAND ----------

# MAGIC %md
# MAGIC Sá»­ dá»¥ng `toPandas` cá»§a DataFrame cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i DataFrame cá»§a PySpark thÃ nh DataFrame cá»§a Pandas.
# MAGIC PhÆ°Æ¡ng thá»©c `toPandas` cÃ³ hiá»‡u nÄƒng khÃ¡ kÃ©m, nÃªn khuyáº¿n nghá»‹ phÆ°Æ¡ng phÃ¡p lÆ°u `pyspark.sql.dataframe.DataFrame` vÃ o file má»™t láº§n rá»“i Ä‘á»c file Ä‘Ã£ lÆ°u báº±ng `pandas`.

# COMMAND ----------

pdf = sdf_sales.toPandas()
pdf

# COMMAND ----------

# MAGIC %md
# MAGIC * `pyspark.sql.dataframe.DataFrame` -> `List`

# COMMAND ----------

rows = sdf_sales.select("*").collect()
rows

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Lazy Evaluation (ÄÃ¡nh giÃ¡ trÃ¬ hoÃ£n)

# COMMAND ----------

# MAGIC %md
# MAGIC CÃ¡c phÆ°Æ¡ng thá»©c cá»§a DataFrame Ä‘Æ°á»£c phÃ¢n loáº¡i lá»›n thÃ nh hai há»‡ thá»‘ng xá»­ lÃ½: `transformation` vÃ  `action`. `transformation` tÆ°Æ¡ng á»©ng vá»›i xá»­ lÃ½ lá»c, Ä‘á»‹nh dáº¡ng, káº¿t há»£p dá»¯ liá»‡u (: `join`, `groupBy`), `action` tÆ°Æ¡ng á»©ng vá»›i xá»­ lÃ½ hiá»ƒn thá»‹ vÃ  lÆ°u dá»¯ liá»‡u (: `show`, `write`).
# MAGIC
# MAGIC Trong lazy evaluation cá»§a Spark, khi thá»±c hiá»‡n `transformation`, chá»‰ Ä‘Äƒng kÃ½ ná»™i dung xá»­ lÃ½ vÃ o scheduler mÃ  khÃ´ng thá»±c hiá»‡n xá»­ lÃ½ thá»±c táº¿, khi `action` Ä‘Æ°á»£c thá»±c hiá»‡n, xá»­ lÃ½ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn DataFrame dá»±a trÃªn ná»™i dung cá»§a scheduler.
# MAGIC
# MAGIC Æ¯u Ä‘iá»ƒm cá»§a lazy evaluation lÃ 
# MAGIC
# MAGIC * CÃ³ thá»ƒ kiá»ƒm soÃ¡t Ä‘á»ƒ dá»¯ liá»‡u khÃ´ng cáº§n thiáº¿t khÃ´ng Ä‘Æ°á»£c táº£i lÃªn memory nhiá»u nháº¥t cÃ³ thá»ƒ
# MAGIC * CÃ³ thá»ƒ tá»‘i Æ°u hÃ³a má»™t loáº¡t xá»­ lÃ½ tá»« transformation Ä‘áº¿n action
# MAGIC
# MAGIC v.v.

# COMMAND ----------

# Äá»‹nh nghÄ©a dá»¯ liá»‡u máº«u
data = [
    ("Alice", 25, "F"),
    ("Bob", 30, "M"),
    ("Charlie", 35, "M"),
    ("David", 40, "M"),
    ("Eve", 45, "F"),
]

# Äá»‹nh nghÄ©a schema
schema = T.StructType(
    [
        T.StructField("name", T.StringType(), True),
        T.StructField("age", T.IntegerType(), True),
        T.StructField("gender", T.StringType(), True),
    ]
)

# Táº¡o DataFrame
sdf_demo = spark.createDataFrame(data, schema)

sdf_demo.display()

# COMMAND ----------

# MAGIC %md
# MAGIC **VÃ­ dá»¥**

# COMMAND ----------

# MAGIC %md
# MAGIC DÆ°á»›i Ä‘Ã¢y thá»±c hiá»‡n xá»­ lÃ½ thuá»™c vá» transformation trÃªn DataFrame.
# MAGIC TrÆ°á»›c khi gá»i xá»­ lÃ½ thuá»™c vá» action, xá»­ lÃ½ thá»±c táº¿ khÃ´ng Ä‘Æ°á»£c thá»±c hiá»‡n mÃ  chá»‰ táº¡o scheduler xá»­ lÃ½.

# COMMAND ----------

# VÃ­ dá»¥ vá» lazy evaluation - Thao tÃ¡c transformation
# 1. ThÃªm cá»™t nhÃ¢n Ä‘Ã´i tuá»•i
sdf_transformed = sdf_demo.withColumn("age_double", F.col("age") * 2)


# 2. Lá»c chá»‰ nam giá»›i
sdf_filtered = sdf_transformed.filter(F.col("gender") == "M")


# 3. Chuyá»ƒn Ä‘á»•i tÃªn thÃ nh chá»¯ hoa
sdf_final = sdf_filtered.withColumn("name_upper", F.upper(F.col("name")))

# ChÆ°a Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ cho Ä‘áº¿n khi action Ä‘Æ°á»£c gá»i
print("CÃ¡c thao tÃ¡c transformation chÆ°a Ä‘Æ°á»£c thá»±c hiá»‡n.")


# Thá»±c sá»± thá»±c hiá»‡n action (vÃ­ dá»¥: hiá»ƒn thá»‹)
# sdf_final.display()

# COMMAND ----------

# MAGIC %md
# MAGIC Táº¡i thá»i Ä‘iá»ƒm gá»i action, táº¥t cáº£ transformer Ä‘Æ°á»£c thá»±c hiá»‡n dá»±a trÃªn ná»™i dung cá»§a scheduler

# COMMAND ----------

# MAGIC %md
# MAGIC Náº¿u khÃ´ng xem xÃ©t lazy evaluation mÃ  tiáº¿p tá»¥c thÃªm transformer vÃ o scheduler, táº¡i thá»i Ä‘iá»ƒm thá»±c hiá»‡n action, xá»­ lÃ½ ráº¥t náº·ng sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn dá»¯ liá»‡u khá»•ng lá»“.
# MAGIC Khuyáº¿n nghá»‹ lÆ°u vÃ o disk sau khi káº¿t há»£p báº£ng: `join` hoáº·c lá»c dá»¯ liá»‡u khá»•ng lá»“, Ä‘á»ƒ cÃ³ thá»ƒ tham chiáº¿u káº¿t quáº£ trung gian vá»›i tá»‘c Ä‘á»™ cao.

# COMMAND ----------

# MAGIC %md
# MAGIC ## BÃ i táº­p Hands-on: PhÃ¢n tÃ­ch dá»¯ liá»‡u bÃ¡n hÃ ng
# MAGIC
# MAGIC **Má»¥c tiÃªu**: XÃ¢y dá»±ng dashboard phÃ¢n tÃ­ch kinh doanh tá»« dá»¯ liá»‡u thÃ´

# COMMAND ----------

# === CHUáº¨N Bá»Š Dá»® LIá»†U CHO BÃ€I Táº¬P ===

# Táº¡o dá»¯ liá»‡u bÃ¡n hÃ ng (1 thÃ¡ng)
extended_sales_data = [
    # Tuáº§n 1
    ("TXN001", "C001", "P001", 2, 15000, "2024-01-01", "S001"),
    ("TXN002", "C002", "P002", 1, 25000, "2024-01-01", "S002"),
    ("TXN003", "C003", "P003", 3, 8000, "2024-01-02", "S001"),
    ("TXN004", "C001", "P004", 1, 45000, "2024-01-03", "S003"),
    ("TXN005", "C004", "P001", 5, 15000, "2024-01-04", "S002"),
    # Tuáº§n 2
    ("TXN006", "C002", "P005", 2, 12000, "2024-01-08", "S001"),
    ("TXN007", "C005", "P002", 1, 25000, "2024-01-09", "S003"),
    ("TXN008", "C003", "P003", 4, 8000, "2024-01-10", "S002"),
    ("TXN009", "C006", "P001", 3, 15000, "2024-01-11", "S001"),
    # Tuáº§n 3
    ("TXN010", "C001", "P002", 2, 25000, "2024-01-15", "S002"),
    ("TXN011", "C004", "P004", 1, 45000, "2024-01-16", "S003"),
    ("TXN012", "C002", "P003", 3, 8000, "2024-01-17", "S001"),
    ("TXN013", "C005", "P005", 4, 12000, "2024-01-18", "S002"),
    # Tuáº§n 4
    ("TXN014", "C003", "P001", 6, 15000, "2024-01-22", "S001"),
    ("TXN015", "C006", "P002", 1, 25000, "2024-01-23", "S003"),
    ("TXN016", "C001", "P004", 2, 45000, "2024-01-24", "S002"),
    ("TXN017", "C004", "P003", 5, 8000, "2024-01-25", "S001"),
    ("TXN018", "C002", "P005", 3, 12000, "2024-01-26", "S003"),
]

# Schema giá»¯ nguyÃªn
sdf_sales_extended = spark.createDataFrame(extended_sales_data, sales_schema)

# ThÃ´ng tin chi tiáº¿t khÃ¡ch hÃ ng
customers_detailed = [
    ("C001", "Nguyen Van A", "VIP", "M", "1990-05-15", "Ha Noi"),
    ("C002", "Tran Thi B", "Gold", "F", "1985-08-22", "Ho Chi Minh"),
    ("C003", "Le Van C", "Silver", "M", "1992-03-10", "Da Nang"),
    ("C004", "Pham Thi D", "VIP", "F", "1988-12-05", "Ha Noi"),
    ("C005", "Hoang Van E", "Gold", "M", "1995-07-18", "Hai Phong"),
    ("C006", "Mai Van F", "Silver", "F", "1987-11-30", "Can Tho"),
]

customers_detailed_schema = T.StructType(
    [
        T.StructField("customer_id", T.StringType(), True),
        T.StructField("customer_name", T.StringType(), True),
        T.StructField("tier", T.StringType(), True),
        T.StructField("gender", T.StringType(), True),
        T.StructField("birth_date", T.StringType(), True),
        T.StructField("city", T.StringType(), True),
    ]
)

sdf_customers_detailed = spark.createDataFrame(
    customers_detailed, customers_detailed_schema
)

# ThÃ´ng tin sáº£n pháº©m
products_extended = [
    ("P001", "iPhone 15 Pro", "Electronics", "Apple", 15000),
    ("P002", "Samsung Galaxy S24", "Electronics", "Samsung", 25000),
    ("P003", "Nike Air Max 270", "Fashion", "Nike", 8000),
    ("P004", "MacBook Pro M3", "Electronics", "Apple", 45000),
    ("P005", "Adidas Ultraboost", "Fashion", "Adidas", 12000),
]

products_extended_schema = T.StructType(
    [
        T.StructField("product_id", T.StringType(), True),
        T.StructField("product_name", T.StringType(), True),
        T.StructField("category", T.StringType(), True),
        T.StructField("brand", T.StringType(), True),
        T.StructField("list_price", T.IntegerType(), True),
    ]
)

sdf_products_extended = spark.createDataFrame(
    products_extended, products_extended_schema
)

print("Dá»® LIá»†U CHO BÃ€I Táº¬P HANDS-ON")
print("=" * 50)
print(f"Sales: {sdf_sales_extended.count()} giao dá»‹ch")
print(f"Customers: {sdf_customers_detailed.count()} khÃ¡ch hÃ ng")
print(f"Products: {sdf_products_extended.count()} sáº£n pháº©m")

# COMMAND ----------

# === BÃ€I Táº¬P 1: DATA PREPARATION & CLEANING ===

print("BÃ€I Táº¬P 1: DATA PREPARATION & CLEANING")
print("=" * 50)

# BÆ°á»›c 1: Táº¡o master dataset
sdf_master = sdf_sales_extended.join(
    sdf_customers_detailed, "customer_id", "left"
).join(sdf_products_extended, "product_id", "left")

# BÆ°á»›c 2: LÃ m sáº¡ch vÃ  enriching data
sdf_clean = (
    sdf_master.withColumn("sale_date", F.to_date("sale_date", "yyyy-MM-dd"))
    .withColumn("total_amount", F.col("quantity") * F.col("unit_price"))
    .withColumn("birth_date", F.to_date("birth_date", "yyyy-MM-dd"))
    .withColumn("age", F.floor(F.datediff(F.current_date(), "birth_date") / 365.25))
    .withColumn("week_of_year", F.weekofyear("sale_date"))
    .withColumn("day_name", F.date_format("sale_date", "EEEE"))
    .withColumn(
        "is_weekend",
        F.when(F.dayofweek("sale_date").isin([1, 7]), "Yes").otherwise("No"),
    )
)

# BÆ°á»›c 3: ThÃªm business logic
sdf_enriched = (
    sdf_clean.withColumn(
        "discount_rate",
        F.when(F.col("tier") == "VIP", 0.10)
        .when(F.col("tier") == "Gold", 0.05)
        .otherwise(0.02),
    )
    .withColumn("discount_amount", F.col("total_amount") * F.col("discount_rate"))
    .withColumn("net_amount", F.col("total_amount") - F.col("discount_amount"))
)

print("Data cleaned and enriched!")
print("ðŸ” Sample cá»§a master dataset:")
sdf_enriched.select(
    "transaction_id",
    "customer_name",
    "product_name",
    "total_amount",
    "discount_amount",
    "net_amount",
    "week_of_year",
).display()

# COMMAND ----------

# === BÃ€I Táº¬P 2: SALES PERFORMANCE ANALYSIS ===

print("BÃ€I Táº¬P 2: SALES PERFORMANCE ANALYSIS")
print("=" * 50)

# 2.1 Tá»•ng quan doanh thu
total_metrics = sdf_enriched.agg(
    F.sum("net_amount").alias("total_revenue"),
    F.avg("net_amount").alias("avg_order_value"),
    F.count("transaction_id").alias("total_orders"),
    F.countDistinct("customer_id").alias("unique_customers"),
).collect()[0]

print("Tá»”NG QUAN KINH DOANH:")
print(f"Tá»•ng doanh thu: {total_metrics['total_revenue']:,.0f} VND")
print(f"GiÃ¡ trá»‹ Ä‘Æ¡n hÃ ng TB: {total_metrics['avg_order_value']:,.0f} VND")
print(f"Tá»•ng sá»‘ Ä‘Æ¡n hÃ ng: {total_metrics['total_orders']}")
print(f"KhÃ¡ch hÃ ng unique: {total_metrics['unique_customers']}")

# 2.2 PhÃ¢n tÃ­ch theo sáº£n pháº©m
sdf_product_performance = (
    sdf_enriched.groupBy("product_name", "category", "brand")
    .agg(
        F.sum("net_amount").alias("revenue"),
        F.sum("quantity").alias("quantity_sold"),
        F.count("transaction_id").alias("orders"),
        F.avg("net_amount").alias("avg_order_value"),
    )
    .withColumn("rank", F.rank().over(W.Window.orderBy(F.desc("revenue"))))
    .orderBy("rank")
)

print("\n TOP Sáº¢N PHáº¨M THEO DOANH THU:")
sdf_product_performance.display()

# 2.3 PhÃ¢n tÃ­ch theo tuáº§n
sdf_weekly_trend = (
    sdf_enriched.groupBy("week_of_year")
    .agg(
        F.sum("net_amount").alias("weekly_revenue"),
        F.count("transaction_id").alias("weekly_orders"),
        F.countDistinct("customer_id").alias("weekly_customers"),
    )
    .orderBy("week_of_year")
)

print("XU HÆ¯á»šNG THEO TUáº¦N:")
sdf_weekly_trend.display()

# COMMAND ----------

# === BÃ€I Táº¬P 3: CUSTOMER ANALYSIS ===

print("BÃ€I Táº¬P 3: CUSTOMER ANALYSIS")
print("=" * 50)

# 3.1 PhÃ¢n tÃ­ch khÃ¡ch hÃ ng chi tiáº¿t
sdf_customer_analysis = (
    sdf_enriched.groupBy(
        "customer_id", "customer_name", "tier", "gender", "age", "city"
    )
    .agg(
        F.sum("net_amount").alias("total_spent"),
        F.count("transaction_id").alias("order_frequency"),
        F.avg("net_amount").alias("avg_order_value"),
        F.countDistinct("product_id").alias("product_variety"),
        F.max("sale_date").alias("last_purchase"),
        F.min("sale_date").alias("first_purchase"),
    )
    .withColumn("customer_lifetime_days", F.datediff("last_purchase", "first_purchase"))
    .withColumn(
        "rank_by_revenue", F.rank().over(W.Window.orderBy(F.desc("total_spent")))
    )
)

print("TOP KHÃCH HÃ€NG:")
sdf_customer_analysis.orderBy("rank_by_revenue").display(10)

# 3.2 PhÃ¢n tÃ­ch theo demographics
sdf_demographic_analysis = (
    sdf_enriched.groupBy("gender", "tier")
    .agg(
        F.sum("net_amount").alias("revenue"),
        F.countDistinct("customer_id").alias("customers"),
        F.avg("net_amount").alias("avg_order_value"),
    )
    .withColumn(
        "revenue_per_customer", F.round(F.col("revenue") / F.col("customers"), 0)
    )
    .orderBy("gender", "tier")
)

print("PHÃ‚N TÃCH THEO GIá»šI TÃNH & TIER:")
sdf_demographic_analysis.display()

# 3.3 RFM Analysis cÆ¡ báº£n
current_date = F.lit("2024-01-31")
sdf_rfm_analysis = sdf_enriched.groupBy("customer_id", "customer_name").agg(
    F.datediff(current_date, F.max("sale_date")).alias("recency"),
    F.count("transaction_id").alias("frequency"),
    F.sum("net_amount").alias("monetary"),
)

print("RFM ANALYSIS:")
sdf_rfm_analysis.orderBy(F.desc("monetary")).display()

# COMMAND ----------

# === BÃ€I Táº¬P 4: ADVANCED ANALYTICS WITH WINDOW FUNCTIONS ===

print("BÃ€I Táº¬P 4: ADVANCED ANALYTICS")
print("=" * 50)

# 4.1 Customer purchase patterns
customer_pattern_window = W.Window.partitionBy("customer_id").orderBy("sale_date")

sdf_customer_patterns = (
    sdf_enriched.withColumn(
        "days_between_purchases",
        F.datediff(
            F.col("sale_date"), F.lag("sale_date", 1).over(customer_pattern_window)
        ),
    )
    .withColumn("purchase_sequence", F.row_number().over(customer_pattern_window))
    .withColumn(
        "running_customer_total", F.sum("net_amount").over(customer_pattern_window)
    )
)

print("CUSTOMER PURCHASE PATTERNS:")
sdf_customer_patterns.filter(F.col("purchase_sequence") > 1).select(
    "customer_name",
    "sale_date",
    "net_amount",
    "days_between_purchases",
    "running_customer_total",
).orderBy("customer_name", "sale_date").display()

# 4.2 Product trend analysis
product_trend_window = W.Window.partitionBy("product_name").orderBy("week_of_year")

sdf_product_trends = (
    sdf_enriched.groupBy("product_name", "week_of_year")
    .agg(
        F.sum("quantity").alias("weekly_quantity"),
        F.sum("net_amount").alias("weekly_revenue"),
    )
    .withColumn(
        "quantity_growth",
        (
            F.col("weekly_quantity")
            - F.lag("weekly_quantity", 1).over(product_trend_window)
        )
        / F.lag("weekly_quantity", 1).over(product_trend_window)
        * 100,
    )
    .withColumn(
        "revenue_growth",
        (
            F.col("weekly_revenue")
            - F.lag("weekly_revenue", 1).over(product_trend_window)
        )
        / F.lag("weekly_revenue", 1).over(product_trend_window)
        * 100,
    )
)

print("PRODUCT WEEKLY TRENDS:")
sdf_product_trends.filter(F.col("quantity_growth").isNotNull()).select(
    "product_name", "week_of_year", "weekly_quantity", "quantity_growth"
).orderBy("product_name", "week_of_year").display()



